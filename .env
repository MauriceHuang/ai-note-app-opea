# Port for accessing the Ollama API
LLM_ENDPOINT_PORT=8008

# Proxy settings (if needed)
http_proxy=
https_proxy=
no_proxy=localhost,127.0.0.1

# The LLM model to use with Ollama
LLM_MODEL_ID=llama3.2:1b

# Host IP address (usually your machine's IP on the network)
host_ip=127.0.0.1